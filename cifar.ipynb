{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tinygrad \n",
    "from tinygrad.extra.datasets import fetch_cifar\n",
    "import time\n",
    "from tinygrad.lazy import Device\n",
    "Device.DEFAULT = \"CUDA\"\n",
    "\n",
    "import cv2\n",
    "start =time.monotonic()\n",
    "X_train, Y_train,= fetch_cifar(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.nn import Linear, Conv2d\n",
    "from tinygrad.nn import optim\n",
    "from tinygrad.tensor import Tensor\n",
    "Tensor.training= True\n",
    "\n",
    "class TinyCIFAR:\n",
    "    def __init__ (self):\n",
    "        self.l1 = Linear(3072,3072, bias=True)\n",
    "        self.l2 = Linear(3072,255,bias=True)\n",
    "        self.l3 = Linear(255,10,bias=True)\n",
    "    def __call__ (self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.leakyrelu()\n",
    "        x = self.l2(x)\n",
    "        x = x.leakyrelu()\n",
    "        x = self.l3(x)\n",
    "        return x.log_softmax()\n",
    "    #here lies the buggers\n",
    "    #maybe a dense just doesnt learn CIFAR? \n",
    "    \n",
    "net = TinyCIFAR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3, 32, 32)\n",
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_trains = X_train.reshape(50000, -1)\n",
    "print(X_trains.shape)\n",
    "from tinygrad.nn.optim import SGD, Adam\n",
    "opt = SGD([net.l1.weight, net.l2.weight, net.l3.weight], lr=3e-2)\n",
    "\n",
    "def cross_entropy(out, Y):\n",
    "  num_classes = out.shape[-1]\n",
    "  YY = Y.flatten().astype(np.int32)\n",
    "  y = np.zeros((YY.shape[0], num_classes), np.float32)\n",
    "  y[range(y.shape[0]),YY] = -1.0*num_classes\n",
    "  y = y.reshape(list(Y.shape)+[num_classes])\n",
    "  y = Tensor(y)\n",
    "  return out.mul(y).mean()\n",
    "\n",
    "# print(Y_train.shape)\n",
    "# print(X_trains[5,:].max())\n",
    "# X_trains = np.divide(X_trains, 256)\n",
    "# print(X_trains[:,:].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step0 | loss: 75.52532196044922 | Accuracy: 0.03125\n",
      "step25 | loss: 2.2995476722717285 | Accuracy: 0.0625\n",
      "step50 | loss: 2.301788091659546 | Accuracy: 0.0625\n",
      "step75 | loss: 2.303720235824585 | Accuracy: 0.0625\n",
      "step100 | loss: 2.303795576095581 | Accuracy: 0.1875\n",
      "step125 | loss: 2.300762414932251 | Accuracy: 0.09375\n",
      "step150 | loss: 2.3074402809143066 | Accuracy: 0.09375\n",
      "step175 | loss: 2.3002336025238037 | Accuracy: 0.09375\n",
      "step200 | loss: 2.2912580966949463 | Accuracy: 0.1875\n",
      "step225 | loss: 2.29720401763916 | Accuracy: 0.09375\n",
      "step250 | loss: 2.298412322998047 | Accuracy: 0.09375\n",
      "step275 | loss: 2.3049111366271973 | Accuracy: 0.09375\n",
      "step300 | loss: 2.2987592220306396 | Accuracy: 0.09375\n",
      "step325 | loss: 2.3082334995269775 | Accuracy: 0.03125\n",
      "step350 | loss: 2.2977683544158936 | Accuracy: 0.21875\n",
      "step375 | loss: 2.3042092323303223 | Accuracy: 0.125\n",
      "step400 | loss: 2.297156810760498 | Accuracy: 0.15625\n",
      "step425 | loss: 2.307393789291382 | Accuracy: 0.03125\n",
      "step450 | loss: 2.3069207668304443 | Accuracy: 0.0625\n",
      "step475 | loss: 2.311094045639038 | Accuracy: 0.03125\n",
      "step500 | loss: 2.298625946044922 | Accuracy: 0.125\n",
      "step525 | loss: 2.2946462631225586 | Accuracy: 0.1875\n",
      "step550 | loss: 2.3073043823242188 | Accuracy: 0.03125\n",
      "step575 | loss: 2.3099443912506104 | Accuracy: 0.03125\n",
      "step600 | loss: 2.3048667907714844 | Accuracy: 0.09375\n",
      "step625 | loss: 2.30012583732605 | Accuracy: 0.125\n",
      "step650 | loss: 2.303619861602783 | Accuracy: 0.125\n",
      "step675 | loss: 2.2954776287078857 | Accuracy: 0.1875\n",
      "step700 | loss: 2.2977280616760254 | Accuracy: 0.09375\n",
      "step725 | loss: 2.297213315963745 | Accuracy: 0.09375\n",
      "step750 | loss: 2.303670644760132 | Accuracy: 0.0625\n",
      "step775 | loss: 2.2952189445495605 | Accuracy: 0.15625\n",
      "step800 | loss: 2.2937114238739014 | Accuracy: 0.21875\n",
      "step825 | loss: 2.29384446144104 | Accuracy: 0.1875\n",
      "step850 | loss: 2.3041975498199463 | Accuracy: 0.125\n",
      "step875 | loss: 2.300992965698242 | Accuracy: 0.09375\n",
      "step900 | loss: 2.301288366317749 | Accuracy: 0.1875\n",
      "step925 | loss: 2.2993738651275635 | Accuracy: 0.15625\n",
      "step950 | loss: 2.3066012859344482 | Accuracy: 0.0625\n",
      "step975 | loss: 2.3038244247436523 | Accuracy: 0.0625\n",
      "step1000 | loss: 2.3097968101501465 | Accuracy: 0.0625\n",
      "step1025 | loss: 2.3084218502044678 | Accuracy: 0.03125\n",
      "step1050 | loss: 2.3046822547912598 | Accuracy: 0.0625\n",
      "step1075 | loss: 2.3083767890930176 | Accuracy: 0.03125\n",
      "step1100 | loss: 2.314880609512329 | Accuracy: 0.0\n",
      "step1125 | loss: 2.3039000034332275 | Accuracy: 0.0625\n",
      "step1150 | loss: 2.3097429275512695 | Accuracy: 0.03125\n",
      "step1175 | loss: 2.3010690212249756 | Accuracy: 0.09375\n",
      "step1200 | loss: 2.3023478984832764 | Accuracy: 0.125\n",
      "step1225 | loss: 2.3081045150756836 | Accuracy: 0.0625\n",
      "step1250 | loss: 2.2976272106170654 | Accuracy: 0.125\n",
      "step1275 | loss: 2.300333023071289 | Accuracy: 0.15625\n",
      "step1300 | loss: 2.3090732097625732 | Accuracy: 0.0625\n",
      "step1325 | loss: 2.3041181564331055 | Accuracy: 0.09375\n",
      "step1350 | loss: 2.311876058578491 | Accuracy: 0.03125\n",
      "step1375 | loss: 2.2994394302368164 | Accuracy: 0.125\n",
      "step1400 | loss: 2.294157028198242 | Accuracy: 0.125\n",
      "step1425 | loss: 2.2971668243408203 | Accuracy: 0.1875\n",
      "step1450 | loss: 2.301182985305786 | Accuracy: 0.09375\n",
      "step1475 | loss: 2.310508966445923 | Accuracy: 0.0625\n",
      "step1500 | loss: 2.3044815063476562 | Accuracy: 0.0625\n",
      "step1525 | loss: 2.308652639389038 | Accuracy: 0.03125\n",
      "step1550 | loss: 2.3091111183166504 | Accuracy: 0.0\n",
      "step1575 | loss: 2.298060894012451 | Accuracy: 0.125\n",
      "step1600 | loss: 2.304245710372925 | Accuracy: 0.09375\n",
      "step1625 | loss: 2.3005030155181885 | Accuracy: 0.15625\n",
      "step1650 | loss: 2.302694320678711 | Accuracy: 0.0625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m batch \u001b[39m=\u001b[39m Tensor(X_trains[\u001b[39m0\u001b[39m], requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m labels \u001b[39m=\u001b[39m Y_train[samp]\n\u001b[0;32m----> 9\u001b[0m out \u001b[39m=\u001b[39m net(batch)\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy(out, labels)\n\u001b[1;32m     12\u001b[0m \u001b[39m#print(loss.numpy())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mTinyCIFAR.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mleakyrelu()\n\u001b[1;32m     16\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml3(x)\n\u001b[0;32m---> 17\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mlog_softmax()\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/tensor.py:404\u001b[0m, in \u001b[0;36mTensor.log_softmax\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_softmax\u001b[39m(\u001b[39mself\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    403\u001b[0m   m, _, ss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_softmax(axis)\n\u001b[0;32m--> 404\u001b[0m   \u001b[39mreturn\u001b[39;00m m \u001b[39m-\u001b[39m ss\u001b[39m.\u001b[39;49mlog()\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/tensor.py:492\u001b[0m, in \u001b[0;36mTensor.log\u001b[0;34m(self)\u001b[0m\n\u001b[0;32m--> 492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog\u001b[39m(\u001b[39mself\u001b[39m): \u001b[39mreturn\u001b[39;00m mlops\u001b[39m.\u001b[39;49mLog\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/tensor.py:28\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(fxn, *x, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(fxn:Type[Function], \u001b[39m*\u001b[39mx:Tensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     27\u001b[0m   ctx \u001b[39m=\u001b[39m fxn(x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice, \u001b[39m*\u001b[39mx)\n\u001b[0;32m---> 28\u001b[0m   ret \u001b[39m=\u001b[39m Tensor(ctx\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m[t\u001b[39m.\u001b[39;49mlazydata \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m x], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), device\u001b[39m=\u001b[39mctx\u001b[39m.\u001b[39mdevice, requires_grad\u001b[39m=\u001b[39mctx\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m     29\u001b[0m   \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mrequires_grad \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor\u001b[39m.\u001b[39mno_grad: ret\u001b[39m.\u001b[39m_ctx \u001b[39m=\u001b[39m ctx    \u001b[39m# used by autograd engine\u001b[39;00m\n\u001b[1;32m     30\u001b[0m   \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/mlops.py:44\u001b[0m, in \u001b[0;36mLog.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x:LazyBuffer) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LazyBuffer:\n\u001b[1;32m     43\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 44\u001b[0m   \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49munary_op(UnaryOps\u001b[39m.\u001b[39;49mLOG2)\u001b[39m.\u001b[39mbinary_op(BinaryOps\u001b[39m.\u001b[39mMUL, x\u001b[39m.\u001b[39mconst_like(math\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m)))\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/lazy.py:194\u001b[0m, in \u001b[0;36mLazyBuffer.unary_op\u001b[0;34m(self, op)\u001b[0m\n\u001b[0;32m--> 194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munary_op\u001b[39m(\u001b[39mself\u001b[39m:LazyBuffer, op:UnaryOps) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LazyBuffer: \u001b[39mreturn\u001b[39;00m elementwise_op(op, \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/lazy.py:318\u001b[0m, in \u001b[0;36melementwise_op\u001b[0;34m(op, arg, *srcs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m MERGE_ELEMENTWISE_OPS:\n\u001b[1;32m    315\u001b[0m   \u001b[39m# remove the buffers from any (childless) BinaryOps that feed into this\u001b[39;00m\n\u001b[1;32m    316\u001b[0m   srcs \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([x\u001b[39m.\u001b[39mop \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39moptype \u001b[39m==\u001b[39m BinaryOps \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mchildren) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m x\u001b[39m.\u001b[39mrealized \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m srcs])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m \u001b[39mreturn\u001b[39;00m create_lazybuffer(out_device, ShapeTracker(out_shape), BinaryOps, LazyOp(op, srcs, arg), out_dtype)\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/lazy.py:106\u001b[0m, in \u001b[0;36mcreate_lazybuffer\u001b[0;34m(device, st, optype, op, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m wop \u001b[39m=\u001b[39m (device, dtype, optype, ref(op))\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m wop \u001b[39min\u001b[39;00m lazycache: \u001b[39mreturn\u001b[39;00m lazycache[wop]\n\u001b[0;32m--> 106\u001b[0m lazycache[wop] \u001b[39m=\u001b[39m ret \u001b[39m=\u001b[39m LazyBuffer(device, st, optype, op, dtype)\n\u001b[1;32m    107\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/lazy.py:118\u001b[0m, in \u001b[0;36mLazyBuffer.__init__\u001b[0;34m(self, device, st, optype, op, dtype, src)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_buffer: Optional[RawBuffer] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m   \u001b[39m# TODO: do we really need this? or can we just use realized\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m# TODO: does children have to be a ref count instead of a set? can a Buffer be a double child?\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren: LightWeakSet \u001b[39m=\u001b[39m LightWeakSet()\n\u001b[1;32m    119\u001b[0m \u001b[39m# NOTE: op should be read only after construction of LazyBuffer\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop: LazyOp \u001b[39m=\u001b[39m op\n",
      "File \u001b[0;32m~/code/TinyNets/venv/lib/python3.10/site-packages/tinygrad/tinygrad/helpers.py:130\u001b[0m, in \u001b[0;36mLightWeakSet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 130\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mset\u001b[39;49m()\n\u001b[1;32m    131\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m_remove\u001b[39m(item, selfref\u001b[39m=\u001b[39mref(\u001b[39mself\u001b[39m)):\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m selfref()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "BS=32\n",
    "\n",
    "#Train the modelieren\n",
    "\n",
    "for step in range(12000):\n",
    "    samp=np.random.randint(0, X_trains.shape[0], BS)\n",
    "    batch = Tensor(X_trains[0], requires_grad=True)\n",
    "    labels = Y_train[samp]\n",
    "    out = net(batch)\n",
    "    \n",
    "    loss = cross_entropy(out, labels)\n",
    "    #print(loss.numpy())\n",
    "    opt.zero_grad() #NOTE:I'm unsure why it's giving me t.grad is not None (it was array shaping)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    #accuracy dealings\n",
    "    pred = np.argmax(out.numpy(), axis=-1)\n",
    "    acc = (pred == labels).mean()\n",
    "    # if acc >= 1: #how optimistic of myself\n",
    "    #     break\n",
    "    if step % 25 == 0:\n",
    "        print(f'step{step} | loss: {loss.numpy()} | Accuracy: {acc}')\n",
    "    if step == 2000: \n",
    "        if acc < 0.25:\n",
    "            print('Training but not learning')\n",
    "            break\n",
    "\n",
    "\n",
    "end=time.monotonic()\n",
    "print(f'{end-start:.3}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 32, 32) (10000,)\n",
      "(10000, 3072)\n",
      "Test Accuracy: 0.094375\n"
     ]
    }
   ],
   "source": [
    "#Validate model\n",
    "X_test, Y_test= fetch_cifar(train=False)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "av_acc = 0 #loopy fucks up without this\n",
    "X_tests = X_test.reshape(10000, -1)\n",
    "print(X_tests.shape)\n",
    "testamount = 100\n",
    "for step in range(testamount):\n",
    "    samp = np.random.randint(0, X_tests.shape[0], size=BS)\n",
    "    batch = Tensor(X_tests[samp], requires_grad=True)\n",
    "    labels = Y_test[samp]\n",
    "    out = net (batch)\n",
    "\n",
    "    pred = np.argmax(out.numpy(), axis=-1)\n",
    "    av_acc += (pred == labels).mean()\n",
    "    # print(out.numpy())\n",
    "    # #print(np.argmax(out.numpy()))\n",
    "    # print(Y_test[samp])\n",
    "    # #print(batch.numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {av_acc / testamount}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
