{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tinygrad \n",
    "from extra.datasets import fetch_cifar\n",
    "import time\n",
    "import cv2\n",
    "start =time.monotonic()\n",
    "X_train, Y_train = fetch_cifar(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn import optim\n",
    "from tinygrad.tensor import Tensor\n",
    "\n",
    "class TinyCIFAR:\n",
    "    def __init__ (self):\n",
    "        self.l1 = Linear(3072,1024, bias=True)\n",
    "        self.l2 = Linear(1024,1024,bias=True)\n",
    "        self.l3 = Linear(1024,10, bias=True)\n",
    "\n",
    "    def __call__ (self, x):\n",
    "        x = self.l1(x)\n",
    "        x = x.leakyrelu()\n",
    "        x = self.l2(x)\n",
    "        x = x.leakyrelu()\n",
    "        x = self.l3(x)\n",
    "        return x.log_softmax()\n",
    "    \n",
    "net = TinyCIFAR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 30720)\n"
     ]
    }
   ],
   "source": [
    "X_trains = X_train.reshape(5000, -1)\n",
    "print(X_trains.shape)\n",
    "Tensor.training= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "can't expand (1024, 3072) into (1024, 30720)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(samp\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m labels \u001b[39m=\u001b[39m Y_train[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m out \u001b[39m=\u001b[39m net(batch)\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m sparse_categorical_crossentropy(out,labels)\n\u001b[1;32m     16\u001b[0m opt\u001b[39m.\u001b[39mzero_grad() \u001b[39m#NOTE:I'm unsure why it's giving me t.grad is not None (it was array shaping)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mTinyCIFAR.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m (\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1(x)\n\u001b[1;32m     13\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mleakyrelu()\n\u001b[1;32m     14\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2(x)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/nn/__init__.py:71\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 71\u001b[0m   \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mlinear(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mtranspose(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:613\u001b[0m, in \u001b[0;36mTensor.linear\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlinear\u001b[39m(\u001b[39mself\u001b[39m, weight:Tensor, bias:Optional[Tensor]\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 613\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmul(weight) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(weight\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdot(weight)\n\u001b[1;32m    614\u001b[0m   \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39madd(bias) \u001b[39mif\u001b[39;00m bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m x\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:479\u001b[0m, in \u001b[0;36mTensor.dot\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m    477\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m*\u001b[39m[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mmin\u001b[39m(n1\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n2\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    478\u001b[0m w \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mreshape(\u001b[39m*\u001b[39mw\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m], \u001b[39m*\u001b[39m[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39mmin\u001b[39m(n1\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, n2\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39m*\u001b[39mw\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39mmin\u001b[39m(n2, \u001b[39m2\u001b[39m):])\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39mmin\u001b[39m(n2, \u001b[39m2\u001b[39m))\n\u001b[0;32m--> 479\u001b[0m \u001b[39mreturn\u001b[39;00m (x\u001b[39m*\u001b[39;49mw)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:584\u001b[0m, in \u001b[0;36mTensor.__mul__\u001b[0;34m(self, x)\u001b[0m\n\u001b[0;32m--> 584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__mul__\u001b[39m(\u001b[39mself\u001b[39m, x) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmul(x)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:558\u001b[0m, in \u001b[0;36mTensor.mul\u001b[0;34m(self, x, reverse)\u001b[0m\n\u001b[0;32m--> 558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmul\u001b[39m(\u001b[39mself\u001b[39m, x:Union[Tensor, \u001b[39mfloat\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor: \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_broadcasted(mlops\u001b[39m.\u001b[39;49mMul, x, reverse) \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m Tensor \u001b[39mor\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:552\u001b[0m, in \u001b[0;36mTensor._broadcasted\u001b[0;34m(self, fxn, other, reverse)\u001b[0m\n\u001b[1;32m    550\u001b[0m shape_ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([\u001b[39mmax\u001b[39m(x, y) \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(x\u001b[39m.\u001b[39mshape, y\u001b[39m.\u001b[39mshape)])\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m shape_ret: x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mexpand(shape_ret)\n\u001b[0;32m--> 552\u001b[0m \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m shape_ret: y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39;49mexpand(shape_ret)\n\u001b[1;32m    554\u001b[0m \u001b[39mreturn\u001b[39;00m fxn\u001b[39m.\u001b[39mapply(x, y)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:238\u001b[0m, in \u001b[0;36mTensor.expand\u001b[0;34m(self, shape, *args)\u001b[0m\n\u001b[0;32m--> 238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpand\u001b[39m(\u001b[39mself\u001b[39m, shape, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor: \u001b[39mreturn\u001b[39;00m mlops\u001b[39m.\u001b[39;49mExpand\u001b[39m.\u001b[39;49mapply(\u001b[39mself\u001b[39;49m, shape\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m([x \u001b[39mif\u001b[39;49;00m x \u001b[39m!=\u001b[39;49m \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39melse\u001b[39;49;00m s \u001b[39mfor\u001b[39;49;00m s,x \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, argfix(shape, \u001b[39m*\u001b[39;49margs))]))\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/tensor.py:28\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(fxn, *x, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(fxn:Type[Function], \u001b[39m*\u001b[39mx:Tensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     27\u001b[0m   ctx \u001b[39m=\u001b[39m fxn(x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice, \u001b[39m*\u001b[39mx)\n\u001b[0;32m---> 28\u001b[0m   ret \u001b[39m=\u001b[39m Tensor(ctx\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m[t\u001b[39m.\u001b[39;49mlazydata \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m x], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), device\u001b[39m=\u001b[39mctx\u001b[39m.\u001b[39mdevice, requires_grad\u001b[39m=\u001b[39mctx\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m     29\u001b[0m   \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mrequires_grad \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor\u001b[39m.\u001b[39mno_grad: ret\u001b[39m.\u001b[39m_ctx \u001b[39m=\u001b[39m ctx    \u001b[39m# used by autograd engine\u001b[39;00m\n\u001b[1;32m     30\u001b[0m   \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/mlops.py:171\u001b[0m, in \u001b[0;36mExpand.forward\u001b[0;34m(self, x, shape)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x:LazyBuffer, shape:ShapeType) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LazyBuffer:\n\u001b[1;32m    170\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 171\u001b[0m   \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mexpand(shape)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/lazy.py:231\u001b[0m, in \u001b[0;36mLazyBuffer.expand\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrealized \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39mop \u001b[39m==\u001b[39m MovementOps\u001b[39m.\u001b[39mEXPAND:\n\u001b[1;32m    230\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39msrc[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mexpand(arg)\n\u001b[0;32m--> 231\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshuffle_and_prune_movement_ops(ShapeTracker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mst)\u001b[39m.\u001b[39;49mexpand(arg), MovementOps\u001b[39m.\u001b[39mEXPAND, arg)\n",
      "File \u001b[0;32m~/fun/TinyNets/venv/lib/tinygrad/tinygrad/shape/shapetracker.py:232\u001b[0m, in \u001b[0;36mShapeTracker.expand\u001b[0;34m(self, new_shape)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpand\u001b[39m(\u001b[39mself\u001b[39m, new_shape: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ShapeTracker:\n\u001b[1;32m    231\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(new_shape) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviews[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 232\u001b[0m   \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mint\u001b[39m) \u001b[39mand\u001b[39;00m (s \u001b[39m==\u001b[39m x \u001b[39mor\u001b[39;00m (s \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m st \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)) \u001b[39mfor\u001b[39;00m s,x,st \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, new_shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviews[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrides)), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt expand \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m into \u001b[39m\u001b[39m{\u001b[39;00mnew_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m   \u001b[39m# NOTE: can the mask ever be (0,0)?\u001b[39;00m\n\u001b[1;32m    234\u001b[0m   mask \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([(((\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m m \u001b[39m!=\u001b[39m (\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m) \u001b[39melse\u001b[39;00m (\u001b[39m0\u001b[39m,ns)) \u001b[39mif\u001b[39;00m s \u001b[39m!=\u001b[39m ns \u001b[39melse\u001b[39;00m m) \u001b[39mfor\u001b[39;00m m,s,ns \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviews[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mmask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, new_shape)]) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviews[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mmask \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: can't expand (1024, 3072) into (1024, 30720)"
     ]
    }
   ],
   "source": [
    "from extra.training import sparse_categorical_crossentropy\n",
    "opt = optim.Adam([net.l1.weight, net.l2.weight, net.l3.weight])\n",
    "\n",
    "\n",
    "BS=32\n",
    "\n",
    "#run the modelieren\n",
    "\n",
    "for step in range(1):\n",
    "    samp=np.random.randint(0, 5000, 1)\n",
    "    batch = Tensor(X_train[0], requires_grad=True)\n",
    "    print(samp.shape)\n",
    "    labels = Y_train[1]\n",
    "    out = net(batch)\n",
    "    loss = sparse_categorical_crossentropy(out,labels)\n",
    "    opt.zero_grad() #NOTE:I'm unsure why it's giving me t.grad is not None (it was array shaping)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    #accuracy dealings\n",
    "    pred = np.argmax(out.numpy(), axis=-1)\n",
    "    acc = pred == labels.mean()\n",
    "    # if acc >= 1:\n",
    "    #     break\n",
    "    if step % 100 == 0:\n",
    "        print(f'step{step} | loss: {loss.numpy()} | Accuracy: {acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
